{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "RsIlA6oNDofz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetropolisHastings import MetropolisHastings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "Current GPU: NVIDIA GeForce GTX 960\n",
      "Total GPU memory: 2.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()  # Get the number of available GPUs\n",
    "    current_gpu = torch.cuda.current_device()  # Get the index of the current GPU\n",
    "    gpu_name = torch.cuda.get_device_name(current_gpu)  # Get the name of the current GPU\n",
    "    \n",
    "    # Get detailed GPU properties\n",
    "    gpu_properties = torch.cuda.get_device_properties(current_gpu)\n",
    "    gpu_total_memory = gpu_properties.total_memory / (1024 ** 3)  # Total memory in GB\n",
    "    \n",
    "    # Print GPU information\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "    print(f\"Total GPU memory: {gpu_total_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available. CUDA is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of x = -0.0017340191407129169\n",
      "Std of x = 1.0007357597351074\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "N = 100000\n",
    "z = torch.zeros(N)\n",
    "dataType = \"Gaussian\"\n",
    "#dataType = \"logNormal\"\n",
    "if dataType == \"Gaussian\":\n",
    "    x = torch.randn(N)\n",
    "elif dataType == \"logNormal\":\n",
    "    x = z.log_normal_(0,1)\n",
    "\n",
    "# We store the mean and std deviation for later reference, they are also the MAP and MLE estimates in this case.\n",
    "realMean = torch.mean(x)\n",
    "realStd = torch.std(x)\n",
    "print(f\"Mean of x = {realMean}\")\n",
    "print(f\"Std of x = {realStd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ihqsvTQ74Vy-"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m vanill \u001b[38;5;241m=\u001b[39m MetropolisHastings(x)\n\u001b[0;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 5\u001b[0m run_vanill \u001b[38;5;241m=\u001b[39m vanill\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;28mmap\u001b[39m, x)\n\u001b[0;32m      6\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\fynno\\Desktop\\bt_MCMC_big_data\\MetropolisHastings.py:7\u001b[0m, in \u001b[0;36mMetropolisHastings.run\u001b[1;34m(self, T, theta, data)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, T, theta, data):\n\u001b[1;32m----> 7\u001b[0m     S \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(T, theta\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      8\u001b[0m     S[\u001b[38;5;241m0\u001b[39m,:] \u001b[38;5;241m=\u001b[39m theta\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "map = torch.tensor([realMean,realStd])\n",
    "vanill = MetropolisHastings(x)\n",
    "\n",
    "start_time = time.time()\n",
    "run_vanill = vanill.run(10000, map, x)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "\n",
    "sns.jointplot(x=run_vanill[:,0],y=run_vanill[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CKtXu_jxNH0I"
   },
   "outputs": [],
   "source": [
    "# class FlyMH(MetropolisHastings):\n",
    "\n",
    "#   def run(self, T, resample_fraction):\n",
    "#       z = torch.zeros(self.N * resample_fraction)\n",
    "#       for i in range(len(z)):\n",
    "#         n = torch.randint(low=1, high=self.N+1, size=(1,))\n",
    "#         z[i] =\n",
    "\n",
    "\n",
    "\n",
    "#       S = torch.zeros(T, self.theta.size(0))\n",
    "#       S[0,:] = self.theta\n",
    "#       for i in range(T-1):\n",
    "#         theta_new = self.get_theta_new(S[i,:])\n",
    "#         log_alpha = self.get_log_alpha(S[i,:],theta_new)\n",
    "#         log_u = torch.log(torch.rand(1))/N\n",
    "#         if log_u < log_alpha:\n",
    "#           S[i+1,:] = theta_new\n",
    "#         else:\n",
    "#           S[i+1,:] = S[i,:]\n",
    "\n",
    "#       return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bUkux-MkM_Rp"
   },
   "outputs": [],
   "source": [
    "# 'class consensusMH(MetropolisHastings):\n",
    "#     def __init__(self, num_batches, dataset):\n",
    "#         super().__init__(dataset)\n",
    "#         self.num_batches = num_batches\n",
    "    \n",
    "#     def create_batches(self):\n",
    "#         batch_size = self.N // self.num_batches  # Calculate batch size\n",
    "\n",
    "#         # Create shuffled indices\n",
    "#         indices = torch.randperm(self.N)\n",
    "\n",
    "#         # Split shuffled indices into batches\n",
    "#         batches = [indices[i*batch_size:(i+1)*batch_size] for i in range(self.num_batches)]\n",
    "\n",
    "#         # Extract batches from input_tensor using shuffled indices\n",
    "#         batches_data = [self.dataset[batch] for batch in batches]\n",
    "#         return batches_data\n",
    "    \n",
    "#     def combine_batch_samples(self, batch_sample_list):\n",
    "#         batch_sample_tensor = torch.stack(batch_sample_list)\n",
    "#         # Calculate the mean along a specific dimension (e.g., dim=0 for averaging across tensors)\n",
    "#         average_sample_tensor = torch.mean(batch_sample_tensor, dim=0)\n",
    "        \n",
    "#         return average_sample_tensor\n",
    "    \n",
    "#     def run_all_batches(self,T, theta):\n",
    "#         if __name__ == \"__main__\":\n",
    "#             with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#                 futures = [executor.submit(self.run, T, theta, batch) for batch in self.create_batches()]\n",
    "#                 batch_sample_list = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "#         S = self.combine_batch_samples(batch_sample_list)\n",
    "        \n",
    "#         return S'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3496310196.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[29], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    for j in range(self.num_batches)\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "class ConsensusMH(MetropolisHastings):\n",
    "    def __init__(self, dataset, num_batches):\n",
    "        self.dataset = dataset\n",
    "        self.N = dataset.size(0)\n",
    "        self.num_batches = num_batches\n",
    "\n",
    "    def run(self, T, theta, data):\n",
    "        if __name__ == '__main__':\n",
    "            S = torch.zeros(T, self.num_batches, theta.size(0))\n",
    "            S[0,:] = theta.repat(self.num_batches, 1)\n",
    "            processes = []\n",
    "            for j in range(self.num_batches):\n",
    "                p = mp.Process(target= run_batch, args=(T, theta, batch))\n",
    "\n",
    "\n",
    "            #     for i in range(T-1):\n",
    "            #         S[i+1,:] = self.mh_step(S[i,:], data)\n",
    "            # return S\n",
    "\n",
    "    def run_batch(self, T, theta, data):\n",
    "        S = torch.zeros(T, theta.size(0))\n",
    "        S[0,:] = theta\n",
    "        for i in range(T-1):\n",
    "            S[i+1,:] = self.mh_step(S[i,:], data)\n",
    "        return S\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "KmfuvFIUImqO",
    "outputId": "341ad5b1-82c2-46a8-f63f-8774e8cb22b2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConsensusMH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cons \u001b[38;5;241m=\u001b[39m ConsensusMH(dataset \u001b[38;5;241m=\u001b[39m x, num_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m S \u001b[38;5;241m=\u001b[39m cons\u001b[38;5;241m.\u001b[39mrun_all_batches(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;28mmap\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ConsensusMH' is not defined"
     ]
    }
   ],
   "source": [
    "cons = ConsensusMH(dataset = x, num_batches=5)\n",
    "\n",
    "start_time = time.time()\n",
    "S = cons.run_all_batches(10000, map)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "sns.jointplot(x=S[:,0],y=S[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
